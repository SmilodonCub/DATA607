---
title: "Data Science for Business Notes"
author: "Bonnie Cooper"
date: "2/2/2020"
output:
  html_document:
    theme: readable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

## Notes from readings in 'Data Science for Business' for DATA607

<br>

### Chapter 1  
### Introduction: Data-Analytic Thinking  
  
  
[*Dream no small dreams for they have no power to move the hearts of men.* ]{style="float:right"}  
\n
[-Johann Wolfgang von Goethe ]{style="float:right"} 
\n
<br><br>  

#### The Ubiquity of Data Opportunities
Exploiting data for ccompetitive advantage
Widest application of data-mining techniques: targeted marketing, online advertising, recommendations for cross-selling.
View business problems from a data persepctive and understand principles of extracting useful knowledge from data.
Intuition + Creativity + Common Sense + Domain Knowledge
Data Science: a set of fundamental principles that guide the extraction of knowledge from data.
Data mining: extraction of knowledge from data via technologies that incorporate data science techiques & principles.  
<br>
Example: Hurricane Frances. Data driven predictions based on shopper purchase histories from previous storms. looking for unusual local demands for products with inclement weather to optimize stocking products at store locations...the pre-hurricane top-selling item was beer & sales of strawberry pop-tarts went up 7 fold. Of course, What goes better with strawberry pop-tarts than a nice cold beer!  
<br>
Example: Predicting Customer Churn
Churn: customers switching from one company to another.
use data reseources to decide which customers should be offered special incentives to stay on after contracts expire.  
<br>  

#### Data Science, Engineering and Data-Driven Decision Making
Data Science involves the principles, processes and techniques for understanding phenomena via automated analysis of data.
Data-Driven Decision (DDD) making: practice of basing decisions on the analysis of data, rather than purely on intuition.
Statistically, the more data driven a firm is, the more productive it is (Brynjolfsson et al 2011)
DDD Type 1: decisions for which 'discoveries' need to be made
Example: Target was interested in whether they could predict whether people were expecting a baby. built a predictive model of how purchase history changes with pregnancy.
DDD Type 2: decisions that repeat, especially at a massive scale
Example: churn
Predictive modelling: focus on particular indicators that correlate in some way with a quantity of interest (who will churn, who will purchase, etc.)
The data analysis was not simply testing a hypothesis, it was searching for something useful...some actionable information to help the company.  
<br>

#### Data Processing and 'Big Data'
Data engineering and processing are critical to support data science, but they are more general; they support data science applications.
Big Data: data sets that are too large and/or too complex for traditional data processing systems. big data technologies are used for dat processing in support of the data mining techniques and other data science activities.
Use of big data technologies is associates with significant additional productivity growth (Tambe 2012)  
<br>

#### Data and Data Science Capability as a Strategic Asset
A fundamental principle of data science: data,and the capability to extract useful knowledge from data, should be regarded as key strategic assets.
The best data science team can yield little value without the appropriate data; the right data often cannot substantially improve decisions without suitable data science talent.
EXample: Signet Banking using predictive modeling to model profitability by aquiring the necessary data at a loss. they conducted an experiment by giving different customers different credit terms to generate profitability data  
<br>

#### Data-Analytic Thinking
Asses whether and how data can improve performance

<style>
div.blue { background-color:#e68cf6; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">  

#### TAKE THE DATA ... EXTRACT PATTERNS ... THAT ARE USEFUL  

</div>  

Fundamental concepts:

1. Extracting useful knowledge from the data to solve business problems can be treated sytematially by following a process with reasonably well-defined stages.
2. From a large mass of dat, information technology can be used to find information descriptive attributes of entities of interest.
3. If you look too hard at a set of data, you will find something. However, it might not generalize beyond the data you are looking at. (overfitting)
4. Formulating data mining sloutions and evaluating the results involves thinking carefully about the context in which tey will be used.
<br><br>

***
<br><br>

### Chapter 2
### Business Problems and Data Science Solutions

Decomposing a data-analytics problem into pieces such that each piece matches a known task for which tools are available

Big basic types of data mining algorithms you just gotta know:

1. Classification & class probability estimation: for each indivisual in a population, which of a small set of classes does the individual belong to. relevant: scoring, or probability estimation: probability that the individual belongs to a certain class. Whether something will happen.
2. Regression (value estimation) attempts to estimate for each individual the numeric value of some variable for that individual. How much something will hapen.
3. Similarity matching. identify similar individuals based on data known about them. (think recommender systems)
4. Clustering. group individuals in a population together by thier similarity.
5. Co-occurence grouping. attempts to find associations between entities based on transactions involving them. (market basket analysis)
6. Profiling. characterize the typical behavior of an individual, group, or population. used to establish behavioral norms for anomaly detection.
7. Link prediction. predict connections betwen data & possibly the strengths as well (e.g. social network connections)
8. Data Reduction. Take a lare set of data and replace it with a smaller set of data that contains much of the important information of the larger set. Data reduction often involves a loss of information, but what is important is the trade-off for improved insight.
9. Causal modeling. Evens/actions that actually influence other events/actions.
<br><br>

#### Supervised vs Unsupervised Methods.

* supervised: target information / feedback is given about a problem/grouping. A supervised technique is given a specific purpose of a grouping -- pridicting a target.
For supervised learning, there must be data on the target. The value for the target variable for an individual is often called the individual's 'label', often one must incur expense to actively label the data. main classes: classification & regression.
* unsupervised: no feedback is given and the algorithm is left to group/categorize on arbitrary criteria. Unsupervised learning produces grouping based on similarity, but there is no guarentee that these similarities will be useful. main classes: similarity matching, link prediction & data reduction

A vitl part in the early stages of the data mining process is (i) to decide whether the line of attack is supervised or unsupervised, to produce a precise definition of a target variable that will be the focus of data mining.
<br><br>

#### Data Mining and Its Results
different things:

1. mining the data to find patterns and build models.
2. using the results of data mining
<br><br>

#### The Data Mining Process
Iteration is the rule rather than the exception

The CRISP data mining process

* Business Understanding. Understand the problem to be solved. Multiple iterations may be necessary for a acceptable solution to appear. Creativity plays a large role here. What exactly do we want to do? How exactly do we want to do it?
* Data Understanding. understand the strengths and limitations of the data
* Data Preparation. Analytic technologies often require data to be in a different form from how the data are provided natuarally, and some cenversion will be necessary. Defining variables is one of the main points at which human creativity, common sense, and business knowledge come in to play. Often the quality of the data mining solution rests on how well the analysts structure the problems and craft the variables
* Modeling. Pattern captering regularities in the data.
* Evaluation. Asses the data mining results rigorously and to gain confidence that they are valid and reliable before moving on.
* Put the model in to real use. "Your model is not the one that data scientists design, it's what the engineers build."
Regardless of whether the deployment is successful, the process often retruns to the Business Understanding phase.
Data mining is an exploratory undertaking closer to research and development than it is to engineering iterating on approaches and strategy.
<br><br>

#### Other Analytics Techniques and Technologies.

1. Statistics
2. Database Querying
3. Data Warehousing
4. Regression Analysis
5. Machine Learning and Data Mining
<br><br>

#### Answering Business Questions.
Who are the most profitable customers?
Is there really a difference between profitable customers and the average customer?
But who really are these customers? Can I characterize them?
Will some particular new costomer be profitable? How much revenue should I expect this customer to generate?


### Chapter 3
### Introduction to predictive modeling: From Correlation to Supervised Segmentation

#### Fundamental concepts: Identifying informative attributes; segmenting data by progressive attribute selection

Data Mining: finding or selecting important informative variables or attributes of the entities described by the data. Information is a quantity that reduces the undertainty about something.

#### Models Induction & Prediction
Model: simplified representation of reality that abstracts away irrelevant information
Prediction: to estimate un unknown value
Descriptive Model: gain an insight into the underlying phenomenon or process
Supervised learning is model creation where the model describes a relationship between a set of variables and a predefined taget variable (labeled data).
dataset/table/worksheet
examples/instances/rows
independant variables/predictors/explanatory variable
target variable/dependant variable
Model Induction: creation of models from data

#### Supervised Segmentation
Segmentation done using values of variables that will be known when the target is not, then these segments can be used to predict the value of the target variable. segmentation may provide a human-understandable set of segmentation patterns

#### Selecting Informative Attributes: want the subset to be as homogeneous as possible with respect to the target variable. However, 
attributes rarely split a group perfectly.
not all attributes are binary
purity measure: how well each attribute slits a set of examples into segments
Entropy: measure of disorder Entropy = p1log(p1)-p2log(p2)-....
Information Gain: how much an attribute improves (decreases) entropy over the whole segmentation it creates. If we were to now the value of this attribute, how much would it increase our knowledge of the value of the target variable?
IG(parent,children)=entropy(parent)-[p(c1)entropy(c1)+...+p(cn)entropy(c2)] where the entropy of each child is weighted by the proportion of instances belonging to that child. Information gain does not require absolute purity, it can be applied to any number of child subsets, and it takes into account the relative sizes of the children giving more weight to larger subsets.
A natural measure  of impurity for numeric values is variance.
Which single attribute is the most useful for distinguishing a categorical designation == which attribute yields the most information gain across this designation


#### Supervised Segmentation with Tree-Structured Models
attribute selection: if we pick a single variable that gives the most information gain, we create a very simple seggregation
The procedure of classification tree induction is a recursive process of divide and conquer, where the goal at each step is to select an attribute to partition the current group into subgroups that are as pure as possible with respect to the target variable
Decision lines/surfaces/boundaries & hyperplanes
Trees as sets of rules. each rule consists of the attribute tests along the path connection with AND.
Probability estimation: use the instance counts at each leaf to compute a class probability estimate (frequency-based estimate), or laplace correction which moderates the influence of leaves with only a few instances



### Chapter 4
### Fitting a Model to Data

Parameter learning / Parametric modeling: start with aspecific model with attributes. the goal is to tune the paramters so that the model fits the data as well as possible.

Linear Discriminant Fxns. linear discriminant discriminates between classes, and the function od the decision boundary  is a linear combination of attributes.
General Linear Model: f( x ) <- w_0 + w_1x_1 + w_2x_2 + ...

Optimizing an Objective Fnx. define an objectve function that represents our goal and can be calculated for a particular set of weights & data.

Linear regression, logistic regression and SVM are all very similar instances of fitting a linear model to data.

Support Vector Machines: linear classifiers with the objective of maximizing the margin between data classes.penalize misclassified data points. the best fit is some balance between a fat margin an a low total error penalty.

Regression via Mathematical Functions. linear regression as an instance of fitting a (linear) model to the data, choose the objective function to optimize and do so with the business application in mind

Loss Functions (Error Penalty) A loss fxn determines how much penalty should be assigned to an instance based on the error in the model's predicted value

Linear regression: how big is the error of the fitted model?: how far away are the estimated values from the true values on the training data? find the minimum sum of errors, generally, find the sum/mean of the squares of the errors (least squares regression). this strongly penalizes very large errors. however, this leaves it very sensitive to the data. an outlier can skew the fit.

Class Probability Estimation and logistic regression.
Estimate the probability that a new instance belongs to the class of interest. Logistic regression is often thought of simply as a model for the probability of class membership.

* probability estimates should be well calibrated.
* probability estimates need to be discriminative

* for probability estimation, logistic regression uses the same linear model as do our linear discriminants for classification and linear regression for estimating numeric target values.
* The output of the logistic regression model is interpreted as the log-odds of class membership.
* log-odds can be translated directly into the probability of class memebership.

Logistic regression is estimating the log-odds or, more losely, the probability of class membership (a numeric quantity) over a categorical class. We consider it to be a class probability estimation model and NOT a regression model despite its name.

Logistic Regression vs Tree Induction
1. Classification trees use decision boundares that are perpendicular to the instance space axes, whereas linear classifiers use decision boundaries of any direction/orientation. This is a direct consequence of the fact that classification trees select a single attribute at a time whereas linear classifiers use a wighted combination of all attributes.
2. A classification tree is a piecewise classifier that segments the instnace space recursively when it has to using a divide and conquer approach. In principle, a decision tree can cut up the instance space arbitrarily finely into very small regions. A linear classifier places a single decision surface through the entire space. It has great freedom in the orientation of the surface, but it is limited to a single division into two segments. this is a direct consequence of there being a single linear equation that uses all of the variables and must fit the entire dta space.

The two most common families of techniques that are based on fitting the parameters of complex nonlinear functions are nonlinear svm & neural networks.
One can think of a neural network as a stack of models. the bottom stack are the original features. from these are learned a variety of relatively simple models (e.g. logistic regressions ) each subsequent layer in the stack applies a simple model (e.g. another logistic regression) to the next layer to build upon.




### Chapter 5
### Overfitting and its Avoidance

[*If you torture the data long enough, it will confess.* ]{style="float:right"}  
\n
[-Nobel Laureate Ronald Coase
<br><br>

Patterns that generalize: predict well for instances that we have not yet observed.
Generalization: property of a model or modeling rocess whereby the model applies to data that were not used to build the model.
Overfitting: finding chance occurances in the data that look like interesting patterns, but do not generalize. ...the model is tailored to the training data at the expense of generalization to previously unseen data points.
There is a fundamental tradeoff between model complexity and the possibility of overfitting.
Generally, there will be mre overfitting as one allows the modelto be more complex.

#### Overfitting Examined
Base Rate Classifier: classifier that always selects the majority class
A procedure that grows trees until the leaves are pure tends to overfit. Need to find a trade off between the extremes of not splitting the data at all and simply using the average target value in the entire dataset, and building a complete tree out until the leaves are pure. Unfortunately, no one has come up with a procedure to determine this exact sweet spot yet...have to rely on empirically based techniques.
As you increase the dimensionality, you can perfectly fit larger and larger sets of arbitrary points.

```{r}
library(ggplot2)
data(iris)
summary(iris)

ggplot(iris, aes(x=Sepal.Width, y=Petal.Width, color=Species)) + 
    geom_point(size=6)
```

```{r}
#SVM
#linear regression
```

Why does performance degrade with overfitting? ...the short answer: when a model gets more complex, it picks up harmful spurious correlations ... these correlations are idiosyncracies of the specific training set used and do not represent characteristics of the populaion in general. the harm occurs when these spurious correlations produce incorrect generalizations in the model

* all model types are susceptible to overfitting effects.
* samples have natural variations, so any subset can lead to overfitting
* there is no general analytic way to determine if a model is overfitting 

#### Cross-Validation
unlike splitting the data into one training set and one hold-out set, cross-validation computes its estimate over all the data by perfroming multiple splits and systematically swapping out samples for testing. Cross-validation begins by splitting a labeled dataset into 'k' folds. Cross-validation then iterates training and testing 'k' times. So, in each iteration, we have (k-1)/k of the data used for training and 1/k used for testing. Each iteration produces one mode. When cross-validation is finishe, every example will have been used only once for testing, but k-1 times for training. At this point we have performance estimates from all k folds and we can compute the average and standard deviation.

#### Learning Curve
a plot of the generalization performance against the amount of training data A learning curve shows the generalization performance -the performance only testing on the data, plotted against the amount of training data.
Fitting graph: a fitting graph shows the generalization performance as well as the performance on the training data, but plotted against model complexity

#### Overfitting avoidance and complexity control

1. Avoiding overfitting with tree induction. tree inductions is very flexible, so it needs a mechanism of control to avoit overfitting. You can either (i) stop growing the tree before it gets too complex or (ii) grow the tree too complex and then 'prune' it back.How few instances to have on a leaf (set number or hypothesis test in information gain)? 
2. General Methods for Avoiding Overfitting. Nested holdout testing: have a subtraining set to model/test how to fit the validation set before applying to the final test set. It is nested because a second holdout procedure is performed on the training set selected by the first holdout procedure.

Avoiding overfitting for parameter optemization
regularization: instead of just optimizing the fit to the data, we optimize some combination of fit and simplicity. 


<br>

### Chapter 6 
### Similarity, Neighbors, and Clusters

the closer two objects are in feature space, the more similar they are.
**Euclidean Distance** we can compute the overall distance by computing the distances of the inidividual dimentions -the individual features in our setting.The distance is just a number -it has no units, and no meaningful interpretation. It is only useful for comparing the similarity of one pair of instances to that of another pair. It turns out that comparing similarities i etremely useful.
**nearest neighbors** most similar instances
- classification with nearest neighbors: using nearest neighbors to predict an unknown feature by majority vote. probability estimation & regression

How many neighbors is too much influence?
**k-NN** where the k refers to the number of neighbors used. if k=n, the entire dataset would be used & would find the average across feature space. for classification this would predict the mojority class of the entire dataset; for regression the average of all target values; for class probability estimation, the 'base rate' probability. the k in a k-NN classifier is a complexity parameter.

**Weighted Scoring** scale the weight by the reciprocal of the square of the distance. when using weighted scoring the exact value of k is much less critical than with majority voting or unweighted averaging.

How to choose k? conduct a cross-validation or other holdout testing on a training set, for a variety of different values of k, search for one that gives the best performance on the training data. then when we have chosen a k, we build a kNN model from the entire training set.

**intelligibility** issues: the justification of a specific decision and the intelligibility of an entire model.

numerical attributes may have vastly differetn ranges and unless they are scaled appropriately the effect of one attribute with a wide range can swamp the effect of another with a much smaller range.

**curse of dimensionality** since all of the attributes contribute to the distance calculations, instance similarity can be confused and misled by the presence of too many irrelevant attributes.

**feature selection** the judicious determination of features that should be included in the data mining model.


### Chapter 7 
### Decision Analytic Thinking: What is a Good Model?
Connecting the results of mining data back to the goal of the undertaking.
Thinking deeply about the needs of the application.

Evaluating Classifiers.
**Classifier Accuracy** = Num Correct/Total = 1-errorRate general measure of classifier performance.
**Confusion Matrix** the confusion matrix separates out the decisions made by the classifier, making explicit how one class is being confused for another. The main diagonal contains the counts of the correct decisions. The errors of the classifier are the false positives and false negatives.

Unbalanced class distributions
How much do we care about Type1/Type2 Errors?

**Expected Value** : The weighted average of the values of the different possible outcomes where the weight given to each value is it's probability of occurrence
EV = p(0)*v(0) + p(1)*v(1) + p(2)*v(2).....
**Cost & Benefit**
probabilities can be estimated form the data, the costs & benefits often cannot.
Givena matrix of cost and benefits and a matrix of probabilities of the outcomes, get the dot product to find the total expected profit.

p(x,y)= p(y)*p(x|y)
Expected Profit = p(p)*[p(Y|p)*p(Y,p) + p(N|p)*c(N,p)] + p(n)*[p(N|n)*b(N,n)+p(Y|n)*c(Y,n)]

make sure that signs are consistent. don't double count (putting a benefit n one cell and a negative cost for the same thing in the other)

**F-measure** 2*(precision*recall)/(precision + recall)
**precision** TP/(TP + FP) = accuracy/cases predicted to be positive
**Sensitivity** TN/(TN + FP) = True negative rate = 1-False positive rate
**Specificity** TP/(TP + FN) = True positive rate

it is important to consider carefully what would be a reasonable baselie against whih to compare model performance

### Evaluation & Baseline performance
**Majority Classifier** a naie classifier that always chooses the majority class of the training dataset.
**Accuracy** (TP + TN)/(P + N)

<br>

## Chapter 8 
### Visualizing Model Performance

Ranking Instead of Classifying



## Chapter 9
### Evidence & Probabilities

Targeting online consumers with advertisements.
**inference:** infering the value of an instances target variable from the values of the instances features
an instance: ex: a consumer
target variable ex: will a consumer book a room after seeing an add?
what features to use to describe the instances?
humans are notoriously bad at estimating the precise strength of evidence

### Combining evidence probabilistically
independence: knowing about one event tells you nothing about the other.
joint probability = probability that both evens will happen p(AB)=P(A)*p(B)
joint probability using conditional probability: the probability of A and B is A times the probability of B given A.
**Bayes' Rule** p(H | E)=p(E | H)*p(H)/p(E)
Bayes rule makes estimating p(H | E) much easier. We need three pieces of information but they're muh easier to estimate with applied domain knowledge.
We can compute the probability of our hypothesis H given some evidence E by instead looking at the probability of the evidence given the hypothesis, as well as the unconditional probabilities of the hypothesis and the evidence.
**p(E | H)**  the probability that someone has red spots given that they have the measels.
**p(H)** probability that someone has measels
**p(E)** probability that someone has red spots.

### Applying Bayes' Rule to Data Science
*The Quantity we want to Compute*
**posterior probability** p(H|E) the probability that the target variable takes on the class of interest after taking the evidence into account
*The Quantities we can obtain*
**prior** p(H) the probability that we would assign the class before seeing any evidence. the prevalence in the population as a whole
**lkelihood** p(E|H) likelihood of seeing the evidence then the H is true
**evidence** p(E) the likelihood of the evidence. how common is the feature

Naive Bayes p(c|E)=(p(e1|c1)*p(e2|c2)... p(ek|ck))/p(E)
Naive Bayes classifier: classifies a new example by estimating the probability that the example belongs to each class and reports the class with the highest probability.
Simplifies so we don't need to calculate p(E)
Naive bayes tends to perform really well because the violation of th independence assumption tends not to hurt classification performance ...the probability will be overestimated for the correct class and underestimated in the incorrect class(es) (because of 'double counting' instances that are not independent).

**incremental learner** does not need to reprocess all past training examples when new training data becomes available.

**Evidence Lift** the amount by which a classifier cencentrates the positive above the negative examples.

