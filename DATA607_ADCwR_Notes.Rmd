---
title: "Introduction to Web Scraping"
author: "Bonnie Cooper"
date: "2/2/2020"
output:
  html_document:
    theme: readable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

## Walk through of Chapter 1 in 'Automated Data Collection with R'

<br>

### Case Study: World Heritage Sites in Danger

Web scraping demo with Wikipedia!'s page of world heritage sites: 
  <http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger>  
  
The table lists information about each site including the name, city, country, geographical coordinates, the type of danger facing the cite, year it was added to the World Heritage Sites list, year that it was considered endangered, etc. In this demo we will investigate how the sites are distributed across the globe by plotting them out on a map. Visualizing the information will make it more intuitive!

But first we need to load the necessary R libraries

```{r, message=F}
#load necessary R libraries
library( stringr )
library( XML )
library( maps )
library(RCurl)
library(httr)
```
<br><br>

Now we will load the data from the website into our R environment:

```{r}
whsURL <- "http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
heritage_parsed <- GET( whsURL )
tables <- readHTMLTable( rawToChar(heritage_parsed$content ), stringAsFactors = FALSE, header=TRUE )
```
<br><br>

If we look at the Wikipedia! site, we can see that the table with the WHSs that are currently in danger has name entries that start with: Abu Mena, Air and Tenere Natural Reserves, Ancient City of Aleppo, ...  
We want to select this table from tables as a dataframe 'danger_table'  
by inspecting the variable, 'tables', in the Global Environment -> Data field of RStudio, we can be pretty sure that it is tables[[2]] that we are interested in, so we will select it and inspect the contents before we start formatting the data any further


```{r}
danger_table <- tables[[ 2 ]]
danger_table <- danger_table[ , c( 1,3,4,6,7 ) ]
colnames( danger_table ) <- c( "name", "locn", "crit", "yins", "yend" )
head( danger_table )
```
<br><br>

OK, the names in the displayed rows of 'danger_table' correspond with the Wikipedia! table of interest. 
Now we can proceed with cleaning the data...

```{r}
#simplify the 'crit' label to be either 'nat' where 'Natural' | 'cult' for everything else
danger_table$crit <- ifelse( str_detect( danger_table$crit, "Natural") == TRUE, "nat", "cult" )
#chnge 'yins' from factor to numeric
danger_table$yins <- as.numeric( levels( danger_table$yins ) )[ danger_table$yins ]
#simplify 'yend' to be just the first you numeric digits of the entry.
charyend <- as.character( levels( danger_table$yend ) )[ danger_table$yend ]
yend_clean <- substr( charyend, start = 1, stop = 4)
danger_table$yend <- as.numeric( yend_clean )
#the 'locn' field is a mess! we just need the X,Y coordinates for lat/long.
#use regex to parse just the info we need as 2 new columns
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract( danger_table$locn, reg_y )
y_coords <- as.numeric( str_sub( y_coords, 3, -2 ) )
danger_table$y_coords <- round( y_coords, 2 )
x_coords <- str_extract( danger_table$locn, reg_x )
x_coords <- as.numeric( str_sub( x_coords, 3, -1 ) )
danger_table$x_coords <- round( x_coords, 2 )
#remove 'locn'
danger_table$locn <- NULL
#display the first several rows of our cleaned up table
head( danger_table )
```
<br><br>

Now to plot the data on a map

```{r}
#we would like to give the different 'crit' labels unique markers on the graph
pch <- ifelse( danger_table$crit == "nat", 19, 2)
#dr 
map( "world", col='darkgrey', lwd=0.5, mar=c( 0.1, 0.1, 0.1, 0.1))
points( danger_table$x_coords, danger_table$y_coords, pch=pch)
box()
```
<br><br>

Plot the distribution of years when World Heritage Sites were put on the list of endangered sites

```{r}
hist( danger_table$yend,
      freq = TRUE,
      xlab = 'Year when site was put on the list of endangered sites',
      main = '')
```
<br><br>

Plot the distribution of timespans between the year of inscription and the year of endangerment of the World Heritsge Sites that are in danger

```{r}
duration <- danger_table$yend - danger_table$yins
hist( duration,
      freq = TRUE,
      xlab = 'Years it took to become an endangered site',
      main = '')
```


### Five steps to guide data collection:

1. Make sure you knoe exactly what kind of information you need.
2. Find out whether there are any data sources on the Web that might provide direct or indirect information on your problem.
3. Develope a theory of the data generation process when looking into potential sources.
4. Balanc advantages and disadvantages of potential data sources.
5. Make a decision!


### Technologies for:

* Disseminating content on the web: HTTP, XML/HTML, JSON, AJAX, Plain Text
* Technologies for information extraction: R, XPath, JSON parsers, Selenium, RegeX
* Technologies for data storage: R, SQL, Binary, plain text.
