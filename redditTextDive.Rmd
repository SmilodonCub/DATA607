---
title: "Text mining Reddit & Indeed: Scraping and Cleaning Data"
author: "Abdellah Ait, Bonnie Cooper, Gehad Gad & David Moste"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br> 

## Introduction

In this project we worked as a team to gather text data to address the question, **"Which are the most valued data science skills?"** Our approach involved scraping data from two very different sources: The job-listing site [Indeed](https://www.indeed.com/) and the content aggregation site [Reddit](https://www.reddit.com/). We are attempting to get two very different perspectives on valued data science skills: one from the perspective of the job market and another from relevant conversations within a subset of the data science community.  
<br>
Here we describe the method used to scrape text data from the two sources as well as the methods used to process and clean the data.  
<br>

**Table of Contents**  

* Libraries used
* Scraping Relevent Reddit Comments
    + Query Reddit for relevant comments with get_reddit()
    + Query Reddit for relevant URLs with reddit_urls() and accessing comments with reddit_content()
    + Combining comments & removing duplicates cases
* Scraping Indeed Text Data    
* Comment cleaning with the text mining library **tm**
    + cleaning Reddit data
    + cleaning Indeed data
* Natural Language Processing with the udpipe library
    + processing Reddit data
    + processing Indeed data

## Libraries used

The library are used at various steps in this file. The use is detailed through out the text.
```{r, message=FALSE}
library( RedditExtractoR )
library( dplyr )
library( tidyr )
library( tm )
library( SnowballC )
library( wordcloud )
library( RColorBrewer )
library( udpipe )
library( reshape2 )
library( ggplot2 )
library( lattice )
library( rvest )
library( tidyverse )
library( SnowballC )
```
<br><br>

## Scraping Relevent Reddit Comments with **redditExtractoR**

Reddit is a popular content aggregations cite with different boards, or 'subreddits' dedicated (and strictly moderated) for specific topics. This describes the methods used to scrape Reddit comments relevent to data science skills from the subreddit r/datascience.

RedditExtractoR is an R library with tools specific for extracting unstructured data from Reddit. RedditExtractoR functions were used to extract text comment data from relevant threads. 

**Goal** Query reddit for threads relevant to 'Data science skills', collect and mine text for insights, chiefly: "What are the most valued data science skills 


### Query Reddit for relevant comments with get_reddit()

get_reddit() function was used in multiple queries withing the subreddit r/datascience to find relevant thread & comment results for such terms as 'data science skills', 'data science tools', 'learning data science', 'data tools' etc.  

This is an example query:  
**closeAllConnections()  
URLs <- get_reddit(  
  search_terms   = "data science skills",  
  cn_threshold = 5,  
  subreddit = 'datascience'  
)**  


The result of a query is a data.frame with 18 features one of them being the comments from relevent threads  

Mulitiple querries were performed and the resulting data.frames were combined with the rbind() function and exported as a .csv file. The file was uploaded to the B. Cooper's github to be accessed here:

```{r}
rTexts <- 'https://raw.githubusercontent.com/SmilodonCub/DATA607/master/allTexts.csv'
rTexts_df <- read.csv( rTexts, stringsAsFactors = F)
dim( rTexts_df )
```


### Query Reddit for relevant URLs with reddit_urls() and accessing comments with reddit_content()   

Comments were also collected with other redditExtractoR methods.
thread URLs were collected using the reddit_urls using the same search criteria that was used with get_reddit()

The data.frames from multiple reddit_URLs() queries were collated with rbind() Next, the a for loop was used to scrape the comments from each URL with the redditExractoR function reddit_content():

Here is an example query with reddit_urls():  
**closeAllConnections()  
URLs <- reddit_urls(  
  search_terms   = "data science skills",  
  cn_threshold = 1,  
  subreddit = 'datascience'  
)  
numComments <- sum( rURLs_df$num_comments )  
allComments <- data.frame( matrix( 0, nrow = numComments, ncol = 1 ) )  
numURLs <- length( rURLs_df$URL )  
IDX <- 1  
Secs <- 3  
for (aURL in seq(1,numURLs)){  
  urlContent <- reddit_content(rURLs_df$URL[aURL], wait_time = 2)  
  Sys.sleep(Secs)  
  closeAllConnections()  
  gc()  
  numComments_thisURL <- length( urlContent$comment )  
  print( numComments_thisURL )  
  if (numComments_thisURL>0){  
    allComments[ IDX:(IDX + numComments_thisURL -1),] <-  
    urlContent$comment  
  }  
  IDX <- IDX + numComments_thisURL  
  print(IDX)  
}**  

The comments that resulted from this approach were exported as a .csv and uploaded to gihub to be accessed here:
```{r}
url1 <- 'https://raw.githubusercontent.com/SmilodonCub/DATA607/master/redditComments.csv'
moreComments_df <- read.csv( url1, stringsAsFactors = F) %>% 
  select(-X ) %>%
  rename( comment = matrix.0..nrow...numComments..ncol...1.)
dim( moreComments_df )
```


### Combining Comments & removing duplicates cases

Multiple queries with similar search terms predictably yielded overlapping results. Therefore, we will create a new data.frame with just the 'comment' feature and remove the duplicates

```{r}
#combine comments from different redditExtractoR methods
allComments_df <- rTexts_df %>% select( comment ) 
allComments_df <- rbind( allComments_df, moreComments_df)
dim( allComments_df )
#remove duplicate comments
allComments_df <- allComments_df %>% unique()
dim( allComments_df )
```
<br><br>  

## Scraping Indeed Text Data
A [python script](https://github.com/dmoste/DATA607/blob/master/Project%203/data_science_scrape.py) that utilizes the selenium and BeautifulSoup packages was used to collect a list of Indeed job add links for Data Science jobs.


<br><br>



## Comment cleaning with the text mining library **tm**
We will now use a text mining library to break the text into word elements, group like elements & calculate their frequency. 

### Cleaning Reddit data

We previously used multiple redditExtractoR methods to build a data.frame of *hopefully* relevant Reddit comments. Now we will use the tm library to reformat the comments to a structure that holds all the unique words across the dataset as well as a count of the occurence frequency. We start for casting the reddit data.frame as a corpus, or a collection of documents. This transformation is necessary for us to apply the tm_map() methods to the text data.

```{r, results='hide', warning=F}
#VectorSource interprets each element as a document & Corpus casts the result as a collection of documents
commentCorpus <- Corpus( VectorSource( allComments_df ) )
#We pipe the corpus through several tm_map() methods
commentCorpus <- commentCorpus %>%
  tm_map(removePunctuation) %>% ##eliminate punctuation
  tm_map(removeNumbers) %>% #no numbers
  tm_map(stripWhitespace) %>%#white spaces
  tm_map(tolower)%>% ##make all words lowercase
  tm_map(removeWords, stopwords("en")) 

#convert the corpus to a matrix to facilitate fursther analysis
commentCorpus_mat <-as.matrix(TermDocumentMatrix( commentCorpus ))
commentCorpus_wordFreq <-sort(rowSums(commentCorpus_mat), decreasing=TRUE)

#visualize the top 15 most frequeny words in the data
top15 <- commentCorpus_wordFreq[1:15]
aplot <- as.data.frame( melt( top15 ) )
aplot$word <- dimnames( aplot )[[1]]
aplot$word <- factor(aplot$word,
                      levels=aplot$word[order(aplot$value,
                                               decreasing=F)])
fig <- ggplot(aplot, aes(x=word, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word in Corpus") + 
  ylab("Count") +
  coord_flip()
print(fig)

#create a data.frame that holds the 1000 most frequent words and the corresponding frequencies
reddit1000 <- commentCorpus_wordFreq[1:1000]
reddit1000_words <- as.data.frame( melt( reddit1000 ) )
reddit1000_words$word <- dimnames( reddit1000_words )[[1]]
reddit1000_words$word <- factor( reddit1000_words$word,
                    levels = reddit1000_words$word[ order( 
                      reddit1000_words$value,decreasing=T)])
```

### Cleaning Indeed data

Here we will access the listings output and use tm methods to clean the data. We will follow the same steps as we did with the Reddit cemments data:

```{r, message=F, warning=F}
# read in the links that R will scrape from a csv and create column names
URL <- 'https://raw.githubusercontent.com/dmoste/DATA607/master/Project%203/data_science_links.csv'
links <- read.csv(URL, header = FALSE)
names(links) <- c("Link")
links$Link <- as.character(links$Link)

# scrape the lists from each link and add the text to a single string (textList)
textList <- c()
for(i in 1:length(links$Link)){
  h_text <- read_html(links[i,]) %>%
    html_nodes("li") %>%
    html_text()
  textList <- rbind(textList, h_text)
}

# create VCorpus from textList
commentCorpus_I <- Corpus(VectorSource(textList))

# manipulte the corpus to remove unwanted information
commentCorpus_I <- commentCorpus_I %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>%
  tm_map(tolower)%>%
  tm_map(removeWords, stopwords("english"))

# find the most used words from the job postings
commentCorpus_I <-as.matrix(TermDocumentMatrix(commentCorpus_I))
commentCorpus_wordFreq_I <-sort(rowSums(commentCorpus_I), decreasing = TRUE)

#create a data.frame that holds the 1000 most frequent words and the corresponding frequencies
indeed1000 <- commentCorpus_wordFreq_I[1:1000]
indeed1000_words <- as.data.frame( melt( indeed1000 ) )
indeed1000_words$word <- dimnames( indeed1000_words )[[1]]
indeed1000_words$word <- factor( indeed1000_words$word,
                    levels = indeed1000_words$word[ order( 
                      indeed1000_words$value,decreasing=T)])
```
<br><br>

## Natural Language Processing with the udpipe library

Here we use the udpipe library to apply some basic Natural Language Processing on the text. We tag the words with part of speach and select for words that are nouns under the assumption that nouns their frequency of occurance will be most informative about the data science skills in the text.

We start by downloading and loading into the R environment an English language model for udpipe to apply to our text

```{r, message=F}
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = model$file_model)
```

### Precessing Reddit data

Now we apply udpipe's English language model to the Reddit text data. The udpipe_annotate() function will process each word and associate several features. For instance, it will tag each word with it's most likely part of speach (e.g. noun, verb etc)

```{r}
reddit1000_processedWords <- udpipe_annotate(udmodel_english, 
                                          reddit1000_words$word )
reddit1000_NLP <- data.frame(reddit1000_processedWords)
head( reddit1000_NLP )
```

Now that the words have been annotated, we can subset the data for the noun with the assumption that nouns will be more informative about data science skills.
```{r}
#remove duplicated word entries (for ambiguous text)
reddit1000_NLP <- reddit1000_NLP[ !duplicated( reddit1000_NLP$doc_id ), ]
#merge two dataframes
reddit1000_NLP$value <- reddit1000_words$value
#Most occuring nouns
nounsReddit <- subset(reddit1000_NLP, upos %in% c("NOUN")) 
#to check if i'm missing anything interesting:
#verbs <- subset(top1000_NLP, upos %in% c("VERB")) 
#adjs <- subset(top1000_NLP, upos %in% c("ADJ")) 
nounsReddit <- nounsReddit %>% group_by( lemma ) %>% 
  summarise( value = sum( value )) %>%
  arrange( desc( value ) )
nounsReddit$lemma <- factor(nounsReddit$lemma, 
                         levels = rev(nounsReddit$lemma))

fig <- ggplot(head(nounsReddit,15), aes(x=lemma, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word") + 
  ylab("Count") +
  coord_flip()
print(fig)
```

Present the results as a word cloud:
```{r}
set.seed(36) #be sure to set the seed if you want to reproduce the same again

wordcloud(words=nounsReddit$lemma, freq=nounsReddit$value, scale=c(3,.5),max.words = 360, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```


### Processing Indeed data

We now apply the same procedures to the equivalent data.frame of Indeed text data.
```{r}
indeed1000_processedWords <- udpipe_annotate(udmodel_english, 
                                          indeed1000_words$word )
indeed1000_NLP <- data.frame(indeed1000_processedWords)
#head( indeed1000_NLP )

#remove duplicated word entries (for ambiguous text)
indeed1000_NLP <- indeed1000_NLP[ !duplicated( indeed1000_NLP$doc_id ), ]
#merge two dataframes
indeed1000_NLP$value <- indeed1000_words$value
#Most occuring nouns
nounsIndeed <- subset(indeed1000_NLP, upos %in% c("NOUN")) 
#to check if i'm missing anything interesting:
#verbs <- subset(top1000_NLP, upos %in% c("VERB")) 
#adjs <- subset(top1000_NLP, upos %in% c("ADJ")) 
nounsIndeed <- nounsIndeed %>% group_by( lemma ) %>% 
  summarise( value = sum( value )) %>%
  arrange( desc( value ) )
nounsIndeed$lemma <- factor(nounsIndeed$lemma, 
                         levels = rev(nounsIndeed$lemma))
#barchart(sentence ~ value, data = head(nouns, 20), col = "cadetblue", 
         #main = "Most occurring nouns", xlab = "Freq")

fig <- ggplot(head(nounsIndeed,15), aes(x=lemma, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word") + 
  ylab("Count") +
  coord_flip()
print(fig)
```

```{r}
set.seed(36) #be sure to set the seed if you want to reproduce the same again

wordcloud(words=nounsIndeed$lemma, freq=nounsIndeed$value, scale=c(3,.5),max.words = 300, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

There are some remarkable similarities between the words in each of the data sets. In both of the histograms, we can see several words in common.

<br><br><br>