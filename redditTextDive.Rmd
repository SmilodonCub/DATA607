---
title: "Text mining Reddit & Indeed for the most valued Data Science skills"
author: "Abdellah Ait, Bonnie Cooper, Gehad Gad & David Moste"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br> 

# Introduction

In this project we worked as a team to gather text data to address the question, **"Which are the most valued data science skills?"** Our approach involved scraping data from two very different sources: The job-listing site [Indeed](https://www.indeed.com/) and the content aggregation site [Reddit](https://www.reddit.com/). Our motivation is to get two very different perspectives on valued data science skills: one from the perspective of the job market and another from relevant conversations within a subset of the data science community.  
<br>
Here we describe the methods used to scrape text data from the two sources, the methods used to process and clean the data, discuss our analysis approaches and present our findings. We have two Analysis directions that we pursue:

1. **Unsupervised Natural Language Processing (NLP) Analysis** This approach makes no assumptions about which data science skills to extract information about from the raw text data. We use the NLP library udpipe to apply a model of the English language to the text scraped from Indeed & Reddit to identify data science skills that occur with the highest frequency.
2. **Supervised Word & Phrase Frequency Analysis** Here we scrape a new set of text from Indeed and search the text with known data science skills terms
<br>


## Table of Contents 

**R Environment**  

* Libraries used

**Unsupervised Natural Language Processing (NLP) Analysis**  

* Scraping Relevent Reddit Comments
    + Query Reddit for relevant comments with get_reddit()
    + Query Reddit for relevant URLs with reddit_urls() and accessing comments with reddit_content()
    + Combining comments & removing duplicates cases
* Scraping Indeed Text Data    
* Comment cleaning with the text mining library **tm**
    + cleaning Reddit data
    + cleaning Indeed data
* Natural Language Processing with the udpipe library
    + processing Reddit data
    + processing Indeed data
* Centralizing datasets in a Relational Database
* Joint Analysis of the Normalized Data

**Supervised Word & Phrase Frequency Analysis**  

* Approach to Scraping Indeed Job Listings
* Putting all the Pieces Together and build our Scraper
* Analyzing frequencies of Data Science Skills words and phrases

**In Closing**  

* Conclusions
* Future Directions


***

## Libraries used

These libraries are used at various steps in this file. The use of prominent libraries are highlighted throughout the text.
```{r, message=FALSE}
library( RedditExtractoR )
library( dplyr )
library( tidyr )
library( tm )
library( SnowballC )
library( wordcloud )
library( RColorBrewer )
library( udpipe )
library( reshape2 )
library( ggplot2 )
library( lattice )
library( rvest )
library( tidyverse )
library( SnowballC )
library( RMySQL )
library( xml2 )
library( stringi )
library( ggpubr )
```
<br><br>

***
<br>

# Unsupervised Natural Language Processing (NLP) Analysis

## Scraping Relevent Reddit Comments with **redditExtractoR**

Reddit is a popular content aggregations cite with different boards, or 'subreddits' dedicated (and strictly moderated) for specific topics. This section describes the methods used to scrape Reddit comments relevent to data science skills from the subreddit r/datascience.

RedditExtractoR is an R library with tools specific for extracting unstructured data from Reddit. RedditExtractoR functions were used to extract text comment data from relevant threads. 

**Goal:** Query reddit for threads relevant to 'Data science skills', collect and mine text for insights, chiefly: "What are the most valued data science skills 


### Query Reddit for relevant comments with get_reddit()

get_reddit() function was used in multiple queries within the subreddit r/datascience to find relevant thread & comment results for such terms as 'data science skills', 'data science tools', 'learning data science', 'data tools' etc.  

An example get_reddit() query:  
**closeAllConnections()  
URLs <- get_reddit(  
  search_terms   = "data science skills",  
  cn_threshold = 5,  
  subreddit = 'datascience'  
)**  


The result of a query is a data.frame with 18 features one of them being the comments from relevent threads  

Mulitiple querries were performed and the resulting data.frames were combined with the rbind() function and exported as a .csv file. The file was uploaded to the B. Cooper's github to be accessed here:

```{r}
rTexts <- 'https://raw.githubusercontent.com/SmilodonCub/DATA607/master/allTexts.csv'
rTexts_df <- read.csv( rTexts, stringsAsFactors = F)
dim( rTexts_df )
```


### Query Reddit for relevant URLs with reddit_urls() and accessing comments with reddit_content()   

Comments were also collected with other redditExtractoR methods.
thread URLs were collected using the reddit_urls using the same search criteria that was used with get_reddit()

The data.frames from multiple reddit_URLs() queries were collated with rbind() Next, the a for loop was used to scrape the comments from each URL with the redditExractoR function reddit_content():

Here is an example query with reddit_urls():  
**closeAllConnections()  
URLs <- reddit_urls(  
  search_terms   = "data science skills",  
  cn_threshold = 1,  
  subreddit = 'datascience'  
)  
numComments <- sum( rURLs_df$num_comments )    
allComments <- data.frame( matrix( 0, nrow = numComments, ncol = 1 ) )  
numURLs <- length( rURLs_df$URL )  
IDX <- 1  
Secs <- 3  
for (aURL in seq(1,numURLs)){  
  urlContent <- reddit_content(rURLs_df$URL[aURL], wait_time = 2)   
  Sys.sleep(Secs)   
  closeAllConnections()   
  gc()  
  numComments_thisURL <- length( urlContent$comment )  
  print( numComments_thisURL )  
  if (numComments_thisURL>0){  
    allComments[ IDX:(IDX + numComments_thisURL -1),] <-  
    urlContent$comment  
  }  
  IDX <- IDX + numComments_thisURL  
  print(IDX)  
}**  

The comments that resulted from this approach were exported as a .csv and uploaded to gihub to be accessed here:
```{r}
url1 <- 'https://raw.githubusercontent.com/SmilodonCub/DATA607/master/redditComments.csv'
moreComments_df <- read.csv( url1, stringsAsFactors = F) %>% 
  select(-X ) %>%
  rename( comment = matrix.0..nrow...numComments..ncol...1.)
dim( moreComments_df )
```
<br>

This supplementary [.R script](https://github.com/SmilodonCub/DATA607/blob/master/reddit_scratch.R) was used for the analysis and is available on B. Cooper's github.

### Combining Comments & removing duplicates cases

Multiple queries with similar search terms predictably yielded overlapping results. Therefore, we will create a new data.frame with just the 'comment' feature and remove the duplicates

```{r}
#combine comments from different redditExtractoR methods
allComments_df <- rTexts_df %>% select( comment ) 
allComments_df <- rbind( allComments_df, moreComments_df)
dim( allComments_df )
#remove duplicate comments
allComments_df <- allComments_df %>% unique()
dim( allComments_df )
```
<br><br>  

***
<br>

## Scraping Indeed Text Data
A [python script](https://github.com/dmoste/DATA607/blob/master/Project%203/data_science_scrape.py) that utilizes the selenium and BeautifulSoup packages was used to collect a list of Indeed job add links for Data Science jobs.

The following scripts accesses the output of the python scrape and formats the text to a single string for downstream processing:
```{r, message=F, warning=F}
# read in the links that R will scrape from a csv and create column names
URL <- 'https://raw.githubusercontent.com/dmoste/DATA607/master/Project%203/data_science_links.csv'
links <- read.csv(URL, header = FALSE)
names(links) <- c("Link")
links$Link <- as.character(links$Link)

# scrape the lists from each link and add the text to a single string (textList)
textList <- c()
for(i in 1:length(links$Link)){
  h_text <- read_html(links[i,]) %>%
    html_nodes("li") %>%
    html_text()
  textList <- rbind(textList, h_text)
}
```
<br><br>



## Comment cleaning with the text mining library **tm**
We will now use a text mining library to break the text into word elements, group like elements & calculate their frequency for both datasets. 

### Cleaning Reddit data

We previously used multiple redditExtractoR methods to build a data.frame of relevant Reddit comments. Now we will use the tm library to reformat the comments to a structure that holds all the unique words across the dataset as well as a count of the occurence frequency. We start by casting the reddit data.frame as a corpus, or a collection of documents. This transformation is necessary for us to apply the tm_map() methods to the text data.

```{r, results='hide', warning=F}
#VectorSource interprets each element as a document & Corpus casts the result as a collection of documents
commentCorpus <- Corpus( VectorSource( allComments_df ) )
#We pipe the corpus through several tm_map() methods
commentCorpus <- commentCorpus %>%
  tm_map(removePunctuation) %>% ##eliminate punctuation
  tm_map(removeNumbers) %>% #no numbers
  tm_map(stripWhitespace) %>%#white spaces
  tm_map(tolower)%>% ##make all words lowercase
  tm_map(removeWords, stopwords("en")) 

#convert the corpus to a matrix to facilitate fursther analysis
commentCorpus_mat <-as.matrix(TermDocumentMatrix( commentCorpus ))
commentCorpus_wordFreq <-sort(rowSums(commentCorpus_mat), decreasing=TRUE)

#visualize the top 15 most frequeny words in the data
top15 <- commentCorpus_wordFreq[1:15]
aplot <- as.data.frame( melt( top15 ) )
aplot$word <- dimnames( aplot )[[1]]
aplot$word <- factor(aplot$word,
                      levels=aplot$word[order(aplot$value,
                                               decreasing=F)])
fig <- ggplot(aplot, aes(x=word, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word in Corpus") + 
  ylab("Count") +
  coord_flip()
print(fig)

#create a data.frame that holds the 1000 most frequent words and the corresponding frequencies
reddit1000 <- commentCorpus_wordFreq[1:1000]
reddit1000_words <- as.data.frame( melt( reddit1000 ) )
reddit1000_words$word <- dimnames( reddit1000_words )[[1]]
reddit1000_words$word <- factor( reddit1000_words$word,
                    levels = reddit1000_words$word[ order( 
                      reddit1000_words$value,decreasing=T)])
```

### Cleaning Indeed data

Here we will access the listings output and use tm methods to clean the data. We will follow the same steps as we did with the Reddit cemments data:

```{r, message=F, warning=F}
# create Corpus from textList
commentCorpus_I <- Corpus(VectorSource(textList))

# manipulte the corpus to remove unwanted information
commentCorpus_I <- commentCorpus_I %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>%
  tm_map(tolower)%>%
  tm_map(removeWords, stopwords("english"))

# find the most used words from the job postings
commentCorpus_I <-as.matrix(TermDocumentMatrix(commentCorpus_I))
commentCorpus_wordFreq_I <-sort(rowSums(commentCorpus_I), decreasing = TRUE)

#create a data.frame that holds the 1000 most frequent words and the corresponding frequencies
indeed1000 <- commentCorpus_wordFreq_I[1:1000]
indeed1000_words <- as.data.frame( melt( indeed1000 ) )
indeed1000_words$word <- dimnames( indeed1000_words )[[1]]
indeed1000_words$word <- factor( indeed1000_words$word,
                    levels = indeed1000_words$word[ order( 
                      indeed1000_words$value,decreasing=T)])
```
<br><br>

***
<br>

## Natural Language Processing with the udpipe library

Here we use the udpipe library to apply some basic Natural Language Processing on the text. We tag the words with part of speach and select for words that are nouns under the assumption that nouns their frequency of occurance will be most informative about the data science skills in the text.

We start by downloading and loading into the R environment an English language model for udpipe to apply to our text

```{r, message=F}
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = model$file_model)
```

### Processing Reddit data

Now we apply udpipe's English language model to the Reddit text data. The udpipe_annotate() function will process each word and associate several features. For instance, it will tag each word with it's most likely part of speach (e.g. noun, verb etc)

```{r}
reddit1000_processedWords <- udpipe_annotate(udmodel_english, 
                                          reddit1000_words$word )
reddit1000_NLP <- data.frame(reddit1000_processedWords)
head( reddit1000_NLP )
```

Now that the words have been annotated, we can subset the data for the noun with the assumption that nouns will be more informative about data science skills.
```{r}
#remove duplicated word entries (for ambiguous text)
reddit1000_NLP <- reddit1000_NLP[ !duplicated( reddit1000_NLP$doc_id ), ]
#merge two dataframes
reddit1000_NLP$value <- reddit1000_words$value
#Most occuring nouns
nounsReddit <- subset(reddit1000_NLP, upos %in% c("NOUN")) 
#to check if i'm missing anything interesting:
#verbs <- subset(top1000_NLP, upos %in% c("VERB")) 
#adjs <- subset(top1000_NLP, upos %in% c("ADJ")) 
nounsReddit <- nounsReddit %>% group_by( lemma ) %>% 
  summarise( value = sum( value )) %>%
  arrange( desc( value ) )
nounsReddit$lemma <- factor(nounsReddit$lemma, 
                         levels = rev(nounsReddit$lemma))

fig <- ggplot(head(nounsReddit,15), aes(x=lemma, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word") + 
  ylab("Count") +
  coord_flip()
print(fig)
```

Present the results as a word cloud:
```{r}
set.seed(36) #be sure to set the seed if you want to reproduce the same again

wordcloud(words=nounsReddit$lemma, freq=nounsReddit$value, scale=c(3,.5),max.words = 360, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```


### Processing Indeed data

We now apply the same procedures to the equivalent data.frame of Indeed text data.
```{r}
indeed1000_processedWords <- udpipe_annotate(udmodel_english, 
                                          indeed1000_words$word )
indeed1000_NLP <- data.frame(indeed1000_processedWords)
#head( indeed1000_NLP )

#remove duplicated word entries (for ambiguous text)
indeed1000_NLP <- indeed1000_NLP[ !duplicated( indeed1000_NLP$doc_id ), ]
#merge two dataframes
indeed1000_NLP$value <- indeed1000_words$value
#Most occuring nouns
nounsIndeed <- subset(indeed1000_NLP, upos %in% c("NOUN")) 
#to check if i'm missing anything interesting:
#verbs <- subset(top1000_NLP, upos %in% c("VERB")) 
#adjs <- subset(top1000_NLP, upos %in% c("ADJ")) 
nounsIndeed <- nounsIndeed %>% group_by( lemma ) %>% 
  summarise( value = sum( value )) %>%
  arrange( desc( value ) )
nounsIndeed$lemma <- factor(nounsIndeed$lemma, 
                         levels = rev(nounsIndeed$lemma))
#barchart(sentence ~ value, data = head(nouns, 20), col = "cadetblue", 
         #main = "Most occurring nouns", xlab = "Freq")

fig <- ggplot(head(nounsIndeed,15), aes(x=lemma, y=value)) + 
  geom_bar(stat="identity") + 
  xlab("Word") + 
  ylab("Count") +
  coord_flip()
print(fig)
```

```{r}
set.seed(36) #be sure to set the seed if you want to reproduce the same again

wordcloud(words=nounsIndeed$lemma, freq=nounsIndeed$value, scale=c(3,.5),max.words = 300, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

**As an initial finding**, there are some remarkable similarities between the words in each of the data sets. In both of the histograms, we can see several words in common. In the word clouds there are multiple! However, we need to compare the datasets directly to explore the similarities further.

<br><br>

***
<br>

## Centralizing Datasets in a Relational Database

The facilitate the joint analysis of Indeed and Reddit text data, both data sets were stored in a MySQL relational database. The SQL script that generates tables can be [accessed here](https://github.com/dmoste/DATA607/blob/master/Project%203/Project%203%20SQL.sql).
<br><br>

## Joint Analysis of the Normalized Data

The datasets have be stored in seperate tables in a MySQL database. We will now query the data and perform an inner join of the two tables.


To start, we establish a connection to the MySQL database:
```{r echo = F, eval = T}
# establish connection
con <- dbConnect(MySQL(), user='root', password='FunLasts4Ever!', dbname='project_3', host='127.0.0.1')
```
```{r echo = T, eval = F}
con <- dbConnect(MySQL(), user='root', password='TypeYourPasswordHere', dbname='project_3', host='127.0.0.1')
```
```{r}
con
```

Now we prepare the data so that we can make direct comparisons between the two datasets. We will do this by querying the SQL datatables through our connection to the MySQL database, processing the data, combining data with an inner join method and tidy the data to facilitate visualization:
```{r}
#query the indeed1 and reddit MySQL datatables with a SELECT statement such that all of the contents are selected and cast as R data.frames
sql <- "SELECT * FROM indeed1"
indeed1 <- dbGetQuery(con, sql)
sql <- "SELECT * FROM reddit"
reddit <- dbGetQuery(con, sql)

#Filter by part of speech for words that are nouns
indeed_data <- indeed1 %>%
  filter(upos == "NOUN")
reddit_data <- reddit %>%
  filter(upos == "NOUN")

#Normlize the results to facilitate a direct comparison of the data.
indeed_data$Indeed <- (indeed_data$count)/sum(indeed_data$count)
reddit_data$Reddit <- (reddit_data$count)/sum(reddit_data$count)

#Make a new data.frame that is the result of an inner join of the indeed and reddit data. This will hold all the nouns that thw two data.frames hold in commonality.
#Then, process the joined data set to filter out low incidence cases.
#tidy the data with the pivot_longer() function to facilitate downstream analysis & visualization.
data_compare <- indeed_data %>%
  inner_join(reddit_data, by = "token") %>% #join common elements
  filter(count.x >100) %>% #filter out low incidence cases
  filter(count.y >100) %>%
  select(-lemma.x, -lemma.y, -upos.x, -upos.y) %>%
  pivot_longer( #tidy the data to a long format
    c("Reddit", "Indeed"),
    names_to = "Database",
    values_to = "Count"
  )
```
<br>  

Visualize the joint dataset
```{r}
ggplot(data_compare, aes(x = reorder(token, Count), y = Count,
                          fill = Database)) +
  geom_bar(position = "dodge", stat = "identity") +
  scale_fill_brewer(palette = "Dark2") +
  labs(x = "Word",
       y = "Usage",
       fill = "Site") +
  coord_flip()
```
<br>

The figure above is interesting because it directly compares the normalized occurances of nouns that were shared in common between the Indeed and Reddit data. Next, we can observe the plot that quantifies the correlation of commonly shared words.
<br>

```{r}
data_compare <- data_compare %>% pivot_wider( names_from = Database, values_from = Count ) 
cor( data_compare$Reddit, data_compare$Indeed)
```
```{r}
ggscatter(data_compare, x= "Reddit", y= "Indeed", 
          add = "reg.line", cor.coef = TRUE, conf.int = TRUE)
```


**Our key finding here:** we observe several nouns that, using our domain knowledge, we identify as data science skills: Experience, python, business, analysis, degree and machine (learning). Our data indicate that these skills examples are represented the most when input is considered from both the job market perspective (Indeed) and the context of the data science community (Reddit). We also observe a strong correlation of occurences in text for the words that are shared in common between the two datasets.

<br><br>

# Supervised Word & Phrase Frequency Analysis
<br>

## Approach to Scraping Indeed Job Listings

Here we perform a seperate scrape of Indeed to build a text dataset independent of the unsupervised dataset. The Objective is to create a data frame that includes the job titles and the job description from Indeed job-listings.

After looking for a data scientist job in specific location (in this case New York, NY), we we copied the link address and stored the URL in a variable called url. Then we used the xml2 package and the read_html function to parse the page. In short, this means that the function will read in the code from the webpage and break it down into different elements ('<div>', '<span>', '<p>', etc.) for you to analyse it.


```{r}
url <- "https://www.indeed.com/jobs?as_and=data+scientist&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&as_src=&salary=&radius=25&l=New+York%2C+NY&fromage=any&limit=50&sort=&psf=advsrch&from=advancedsearch"
page <- read_html(url)
```


**The Job Titles **  
By inspecting the code in the Indeed website using Inspect element tool we see that the The job title is located under the anchor tag. If we look more into it we can also see the it is located under the **jobtitle** CSS selector . 

**Job Descriptions**  
Youâ€™ll notice, that on the current page, there is just a little short description of the job summary. However, we want to get the full description of how many years of experience we need, what skill set is required, and what responsibilities the job entails.

We start by collecting the links on the website. After that we can locate where the job description is located in the document. after inspecting the full description we noticed that the job description is in a <span> element with a class attribute values **.jobsearch-JobComponent-description**. 
Also we'll need our scraper to include the part the will scrape multiple page results. since the only thing that change in the url when moving from page to another is the number or results, so can scrape multiple pages by messing with this number.


### Putting all the Pieces Together and build our Scraper

```{r}
# We changed the number of results per page from 10 to 50 results per page 
first_page <- 50 # first page of result 
last_page <-  500 # last page of results
results <- seq(from = first_page, to = last_page, by = 50)
full_df <- data.frame()
for(i in seq_along(results)) {
  
  first_page_url <- "https://www.indeed.com/jobs?as_and=data+scientist&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&as_src=&salary=&radius=25&l=New+York%2C+NY&fromage=any&limit=50&sort=&psf=advsrch&from=advancedsearch"
  url <- paste0(first_page_url, "&start=", results[i])
  page <- xml2::read_html(url) 
  
Sys.sleep(3) # to avoids error messages such as "Error in open.connect
##Job Title
JobTitle <- page %>% 
rvest::html_nodes('[data-tn-element="jobTitle"]') %>%
rvest::html_attr("title")
## Job Link
links <- page %>% 
  rvest::html_nodes('[data-tn-element="jobTitle"]') %>%
  rvest::html_attr("href")
## Job Description
job_description <- c()
  for(i in seq_along(links)) {
    
    url <- paste0("https://www.indeed.com/", links[i])
    page <- xml2::read_html(url)
    
job_description[[i]] <- page %>%
  html_nodes('.jobsearch-JobComponent-description') %>% 
  html_text() %>%
  stri_trim_both()
  }
}
df <- data.frame(JobTitle, job_description)
  full_df <- rbind(full_df, df) %>% 
    
mutate_at(vars(JobTitle, job_description), as.character)
#full_df_count <- str_count(full_df$job_description, "SQL" )
head(full_df)
```
<br>

### Analyzing frequencies of Data Science Skills words and phrases

Here we use our domain knowledge of the Data Science field to perform a top-down search for key skills that we know to be valuable in data science.
```{r}
# In this part I'll search and count some words related to data science using mutate and str_count
skills <- full_df %>%
mutate(mathematics = str_count(full_df$job_description, "mathematics" )) %>%
mutate(SQL = str_count(full_df$job_description, "SQL" )) %>%
mutate(Python = str_count(full_df$job_description, "Python" )) %>%
mutate(programming = str_count(full_df$job_description, "programming" )) %>%
mutate(Hadoop = str_count(full_df$job_description, "Hadoop" )) %>%
mutate(statistics = str_count(full_df$job_description, "statistics" )) %>%
mutate(mathematics = str_count(full_df$job_description, "mathematics" )) %>%
mutate(modeling = str_count(full_df$job_description, "modeling" )) %>%
mutate(communication = str_count(full_df$job_description, "communication" )) %>%
mutate(Java = str_count(full_df$job_description, "Java" )) %>%
mutate(Apache = str_count(full_df$job_description, "Apache" )) %>%
mutate(Tableau = str_count(full_df$job_description, "Tableau" )) %>%
mutate(computer_science = str_count(full_df$job_description, "computer science" )) %>%
mutate(TensorFlow = str_count(full_df$job_description, "TensorFlow" )) %>%  
mutate(big_data = str_count(full_df$job_description, "big data" )) %>% 
mutate(machine_learning = str_count(full_df$job_description, "machine learning" )) %>%
mutate(SAS = str_count(full_df$job_description, "SAS" )) %>% 
mutate(R = str_count(full_df$job_description, "R" )) %>%  
select(3:19) %>% summarise_all(funs(sum))
head(skills)
```
<br>  

Display the result in a table format:
```{r}
table <- gather(skills, "skill", "Count", 1:16) %>% 
  arrange( Count) %>%
  mutate(skill = factor( skill, skill))
table
```

Visualize as a bar plot:
```{r}
p7 <- ggplot(data=table, aes(x=skill, y=Count)) + geom_bar(stat="identity") + coord_flip()
p7
```
<br>  

From the barplot figure above, **our finding** is that the data science skills words and phrases of interest were well represented in the text data. In particular, we observe hard skills like machine learning, Python and SQL occured frequently. However, we also see that some soft skills (e.g. communication) are very common as well. Additionally, **another key finding** was that several of the terms used here had corresponded frequent occurances in the unsupervised NLP analysis. For example python, and machine (learining).



# In Closing
<br>

## Conclusions

For this project, we scraped, wraggled and processed text data from both Indeed and Reddit. We believe that these sources give two different approaches to addressing which skills are the most valuable in data science. 

Based on the analysis of our data, it appears that some of the most valued skills for data scientists are python, analysis, modeling, machine (learning), analytics, and teamwork. These words came up frequently in every database we examined and make sense in terms of what we have learned about so far. When attempting to get a job, it looks like experience and degree are also valued (though these are not skills, per se).

## Future Directions

1. It would be very interesting to see how this data correlates with text data sets from other sources. Perhaps from other groups for this Project 3?...

2. For our analysis, we only looked at single words. We could revisit the data with RAKE udpipe methods for phrases or word pairings (e.g. 'machine learning' or 'big data')

3. For the unsupervised approach to analyzing the text data, we made the assumption that nouns would be more informative informative than other parts of speech. This was based on subjectively screening the adjectives, verbs etc. in the data. However, we could take measures to verify this assumption.
<br><br><br>